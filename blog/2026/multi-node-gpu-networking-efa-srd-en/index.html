<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Multi-Node P/D Disagg vLLM Serving: How EFA Works Compared to InfiniBand? | Lucas Sangdae Nam </title> <meta name="author" content="Lucas Sangdae Nam"> <meta name="description" content="Multi-node GPU communication on AWS EFA, InfiniBand vs EFA comparison, and vLLM P/D Disagg setup"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8C%89&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://nsd9696.github.io/blog/2026/multi-node-gpu-networking-efa-srd-en/"> <script src="/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Lucas</span> Sangdae Nam </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> <li class="toggle-container"> <a class="nav-link" href="/ko/" title="한국어">KO</a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Multi-Node P/D Disagg vLLM Serving: How EFA Works Compared to InfiniBand?</h1> <p class="post-meta"> Created on February 22, 2026 </p> <p class="post-tags"> <a href="/blog/2026"> <i class="fa-solid fa-calendar fa-sm"></i> 2026 </a>   ·   <a href="/blog/tag/gpu"> <i class="fa-solid fa-hashtag fa-sm"></i> gpu</a>   <a href="/blog/tag/networking"> <i class="fa-solid fa-hashtag fa-sm"></i> networking</a>   <a href="/blog/tag/efa"> <i class="fa-solid fa-hashtag fa-sm"></i> efa</a>   <a href="/blog/tag/infiniband"> <i class="fa-solid fa-hashtag fa-sm"></i> infiniband</a>   <a href="/blog/tag/rdma"> <i class="fa-solid fa-hashtag fa-sm"></i> rdma</a>   <a href="/blog/tag/vllm"> <i class="fa-solid fa-hashtag fa-sm"></i> vllm</a>   ·   <a href="/blog/category/infrastructure"> <i class="fa-solid fa-tag fa-sm"></i> infrastructure</a> </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h2"> <a href="#1-multi-node-gpu-requirements">1. Multi-Node GPU Requirements</a> <ul> <li class="toc-entry toc-h3"><a href="#the-limitations-of-tcpip">The Limitations of TCP/IP</a></li> <li class="toc-entry toc-h3"><a href="#infiniband">InfiniBand</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#2-efa-and-srd">2. EFA and SRD</a> <ul> <li class="toc-entry toc-h3"><a href="#efa-elastic-fabric-adapter">EFA (Elastic Fabric Adapter)</a></li> <li class="toc-entry toc-h3"><a href="#ethernet-vs-infiniband">Ethernet vs InfiniBand</a></li> <li class="toc-entry toc-h3"><a href="#rdma-transport-modes">RDMA Transport Modes</a></li> <li class="toc-entry toc-h3"><a href="#why-rd-is-not-actually-used">Why RD Is Not Actually Used</a></li> <li class="toc-entry toc-h3"><a href="#srd-scalable-reliable-datagram">SRD (Scalable Reliable Datagram)</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#3-efa--gpudirect-rdma">3. EFA → GPUDirect RDMA</a> <ul> <li class="toc-entry toc-h3"><a href="#infiniband-vs-efa-performance-comparison">InfiniBand vs EFA Performance Comparison</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#4-pd-disagg-vllm-serving-on-efa">4. P/D Disagg vLLM Serving on EFA</a> <ul> <li class="toc-entry toc-h3"><a href="#kv-cache-transfer-software-stack">KV Cache Transfer Software Stack</a></li> <li class="toc-entry toc-h3"><a href="#nixl-nvidia-inference-xfer-library">NIXL (NVIDIA Inference Xfer Library)</a></li> <li class="toc-entry toc-h3"><a href="#ucx-unified-communication-x">UCX (Unified Communication X)</a></li> <li class="toc-entry toc-h3"><a href="#libfabric-open-fabrics-interface">libfabric (Open Fabrics Interface)</a></li> <li class="toc-entry toc-h3"><a href="#aws-ofi-nccl-optional">aws-ofi-nccl (optional)</a></li> <li class="toc-entry toc-h3"><a href="#efa-nv-peermem">efa-nv-peermem</a></li> </ul> </li> </ul> </div> <hr> <div id="markdown-content"> <h2 id="1-multi-node-gpu-requirements">1. Multi-Node GPU Requirements</h2> <p>When GPUs communicate within a single node, NVLink provides hundreds of GB/s of bandwidth. NVLink enables direct point-to-point connections between GPUs, allowing them to communicate without going through the CPU.</p> <p>The problem arises when you go multi-node — the bottleneck shifts to the inter-node network. This is especially critical for AllReduce in Tensor Parallelism during LLM serving and KV cache transfer in Disaggregated Serving (Prefill/Decode separation). Naturally, TCP/IP can’t meet these demands.</p> <h3 id="the-limitations-of-tcpip">The Limitations of TCP/IP</h3> <p>TCP requires every packet to traverse the kernel network stack. This triggers a chain of system calls, context switches, protocol processing, and buffer copies, making it extremely slow. Unnecessary memcopy operations also occur in the process.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>TCP/IP Path (Traditional):
GPU → cudaMemcpy → CPU RAM → send() → Kernel → NIC
  → Network →
NIC → Kernel → recv() → CPU RAM → cudaMemcpy → GPU

RDMA + GPUDirect Path:
GPU HBM → NIC (Direct DMA from GPU memory)
  → Network →
NIC → GPU HBM (Direct DMA)
</code></pre></div></div> <p>RDMA (Remote Direct Memory Access) bypasses the CPU and kernel, allowing the NIC to directly access memory. OS Bypass eliminates system call and context switch overhead. GPUDirect RDMA takes this a step further by enabling the NIC to perform DMA directly to GPU memory (HBM).</p> <h3 id="infiniband">InfiniBand</h3> <p>InfiniBand is what made GPUDirect RDMA possible. With each generation — HDR (A100 standard) and NDR (H100 standard) — per-port bandwidth roughly doubles (200Gbps → 400Gbps).</p> <hr> <h2 id="2-efa-and-srd">2. EFA and SRD</h2> <h3 id="efa-elastic-fabric-adapter">EFA (Elastic Fabric Adapter)</h3> <p>EFA (Elastic Fabric Adapter) is a high-performance network interface designed by AWS — think of it as something similar to InfiniBand. It’s available on specific instance types (p4d, p5, p6, etc.) and provides the OS bypass and RDMA capabilities described above.</p> <h3 id="ethernet-vs-infiniband">Ethernet vs InfiniBand</h3> <p>The biggest difference with EFA is that it operates on standard Ethernet fabric. InfiniBand is a single-vendor ecosystem from NVIDIA (Mellanox) — switches, NICs, and cables all need to be Mellanox products, with virtually no alternatives. Ethernet equipment, on the other hand, is available from multiple vendors like Broadcom and Intel, making it commodity hardware.</p> <h3 id="rdma-transport-modes">RDMA Transport Modes</h3> <table> <thead> <tr> <th>Mode</th> <th>Dedicated Connection</th> <th>Delivery Guarantee</th> <th>Order Guarantee</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td> <strong>RC</strong> (Reliable Connection)</td> <td>Required (dedicated QP per peer)</td> <td>O</td> <td>O</td> <td>Default InfiniBand transport</td> </tr> <tr> <td> <strong>UD</strong> (Unreliable Datagram)</td> <td>Not required (single QP)</td> <td>X</td> <td>X</td> <td>Lightest weight</td> </tr> <tr> <td> <strong>RD</strong> (Reliable Datagram)</td> <td>Not required (single QP)</td> <td>O</td> <td>O</td> <td>Theoretically ideal but unimplemented</td> </tr> <tr> <td> <strong>SRD</strong> (Scalable Reliable Datagram)</td> <td>Not required</td> <td>O</td> <td>X (SW reordering)</td> <td>Designed by AWS</td> </tr> </tbody> </table> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>RC (Reliable Connection):
Server A ──dedicated link──→ Server B
Server A ──dedicated link──→ Server C
→ Requires a dedicated QP (Queue Pair) per peer

UD (Unreliable Datagram):
Server A ──link──→ can send to anyone
→ No dedicated link needed, one QP for all peers

RD (Reliable Datagram):
Server A ──link──→ can send to anyone
→ No dedicated link + delivery guarantee + order guarantee
</code></pre></div></div> <h3 id="why-rd-is-not-actually-used">Why RD Is Not Actually Used</h3> <p>In practice, InfiniBand doesn’t really use RD. The reason comes down to hardware implementation complexity.</p> <ul> <li> <strong>RC</strong> only needs to track state for one peer per QP, and <strong>UD</strong> doesn’t need state tracking at all since it’s fire-and-forget.</li> <li>But with <strong>RD</strong>, the NIC must track state for every communication peer: <ul> <li>“Sent up to packet 3 to Server B, received ACK up to 2”</li> <li>“Sent up to packet 7 to Server C, received ACK up to 5”</li> <li>“Sent packet 1 to Server D, no ACK yet”</li> <li>“Sent up to packet 12 to Server E, received ACK up to 10”</li> <li>… × every communication peer</li> </ul> </li> <li>For example, if a single QP communicates with 1,000 servers, the NIC must manage 1,000 states. These need to be stored in the NIC chip’s SRAM, which is expensive and small.</li> <li>So from NVIDIA’s perspective, RC and UD were sufficient, there was no demand for RD, and the cost-benefit didn’t make sense — so it was deprecated.</li> </ul> <h3 id="srd-scalable-reliable-datagram">SRD (Scalable Reliable Datagram)</h3> <p>So what’s different about AWS’s SRD? <strong>It gives up the ordering guarantee from RD.</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>RD:   Packet 1 → Packet 2 → Packet 3  (must arrive in order)
      Path: A ━━━━━━━━━━━━━→ B  (single path)

SRD:  Packet 1 ──Path A──→ ┐
      Packet 2 ──Path B──→ ├→ Software reorders after arrival
      Packet 3 ──Path C──→ ┘
</code></pre></div></div> <p>By giving up ordering, packets can be sprayed across multiple paths. If one path is congested, packets can take another — this is advantageous in Ethernet environments. In environments where thousands of servers share the network, SRD provides stability, which is why it was adopted.</p> <hr> <h2 id="3-efa--gpudirect-rdma">3. EFA → GPUDirect RDMA</h2> <p>When using EFA, the GPUDirect RDMA flow looks like this:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/efa_blog_1-480.webp 480w,/assets/img/efa_blog_1-800.webp 800w,/assets/img/efa_blog_1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/efa_blog_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">EFA GPUDirect RDMA Flow</figcaption> </figure> <p>Of course, intra-node GPU communication still uses NVLink.</p> <h3 id="infiniband-vs-efa-performance-comparison">InfiniBand vs EFA Performance Comparison</h3> <p>So does EFA actually deliver comparable performance to InfiniBand? Here’s a summary of benchmark comparisons:</p> <table> <thead> <tr> <th>Metric</th> <th>InfiniBand</th> <th>EFA/Ethernet</th> <th>Conclusion</th> </tr> </thead> <tbody> <tr> <td><strong>Small message latency</strong></td> <td>~1 µs</td> <td>~10 µs</td> <td>IB dominant</td> </tr> <tr> <td><strong>Large transfer bandwidth</strong></td> <td>~200 Gbps</td> <td>~200 Gbps</td> <td>Similar</td> </tr> <tr> <td><strong>AI Training (large-scale)</strong></td> <td>Baseline</td> <td>Similar with proper tuning</td> <td>Minimal gap</td> </tr> <tr> <td><strong>AI Inference (Decode)</strong></td> <td>Favorable</td> <td>Avg 1.0166% slower</td> <td>IB slightly better</td> </tr> <tr> <td><strong>Cost</strong></td> <td>1.5–2.5x more expensive</td> <td>Baseline</td> <td>Ethernet wins</td> </tr> </tbody> </table> <blockquote> <p>Sources: <a href="https://www.wwt.com/blog/the-battle-of-ai-networking-ethernet-vs-infiniband" rel="external nofollow noopener" target="_blank">WWT - The Battle of AI Networking</a>, <a href="https://www.vitextech.com/blogs/blog/infiniband-vs-ethernet-for-ai-clusters-effective-gpu-networks-in-2025" rel="external nofollow noopener" target="_blank">Vitex Tech - InfiniBand vs Ethernet</a></p> </blockquote> <p>Overall, InfiniBand appears to have better raw performance. However, if you already have your setup on AWS, EFA can be advantageous. That said, considering that recent neo-cloud GPU pricing is significantly cheaper compared to AWS, InfiniBand might be the better choice.</p> <hr> <h2 id="4-pd-disagg-vllm-serving-on-efa">4. P/D Disagg vLLM Serving on EFA</h2> <p>In my case, I was working on setting up Prefill/Decode Disagg in an A100 environment. In this scenario, EFA is directly involved in the kv_transfer stage.</p> <h3 id="kv-cache-transfer-software-stack">KV Cache Transfer Software Stack</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/efa_blog_2-480.webp 480w,/assets/img/efa_blog_2-800.webp 800w,/assets/img/efa_blog_2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/efa_blog_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">KV Cache Transfer Software Stack on EFA</figcaption> </figure> <p>Let’s look at each component above EFA in detail.</p> <h3 id="nixl-nvidia-inference-xfer-library">NIXL (NVIDIA Inference Xfer Library)</h3> <p>NIXL is a dedicated library for GPU-to-GPU memory transfer. The existing NCCL is optimized for collective communication like AllReduce and All-to-All, but it’s not well-suited for point-to-point transfers of memory blocks from one specific GPU to another — that’s why NIXL was created.</p> <p>It’s used in P/D Disagg for the <code class="language-plaintext highlighter-rouge">Prefill GPU ──RDMA Write──▶ Decode GPU</code> process, and for migration between Decode instances: <code class="language-plaintext highlighter-rouge">Decode GPU (Server A, overloaded) ──RDMA──▶ Decode GPU (Server B, available)</code>.</p> <p>NIXL includes a NIXL Agent that handles memory registration, metadata management (GPU memory addresses, RDMA keys, NIC addresses, etc.), and plugin backends (UCX, libfabric) that perform the actual transfers.</p> <h3 id="ucx-unified-communication-x">UCX (Unified Communication X)</h3> <p>UCX is a general-purpose communication framework originally built for InfiniBand. It internally supports protocols including RC, UD, TCP, cuda_ipc (for GPUs on the same node), and cuda_copy (GPU ↔ CPU copy).</p> <p>On EFA, you can use UCX by setting <code class="language-plaintext highlighter-rouge">UCX_TLS=ib</code>, since EFA provides an ibverbs-compatible interface.</p> <ul> <li><a href="https://github.com/amzn/amzn-drivers/blob/master/kernel/linux/efa/src/efa_verbs.c" rel="external nofollow noopener" target="_blank">EFA ibverbs implementation (AWS driver)</a></li> <li><a href="https://github.com/linux-rdma/rdma-core/blob/master/providers/efa/efa.c" rel="external nofollow noopener" target="_blank">rdma-core EFA provider</a></li> </ul> <h3 id="libfabric-open-fabrics-interface">libfabric (Open Fabrics Interface)</h3> <p>libfabric is the standard abstraction layer for network hardware vendors. It’s composed internally of providers including EFA provider, TCP provider, and SHM provider.</p> <p>Officially, libfabric appears to be the recommended path, but in practice UCX had better dependency compatibility. With libfabric, I kept running into GPU memory bad address issues, so I ended up deprecating it for my use case.</p> <h3 id="aws-ofi-nccl-optional">aws-ofi-nccl (optional)</h3> <p>Since my setup was P/D Disagg, I didn’t need NCCL. However, if your model is large enough to require multi-node setup, or for multi-node training, NCCL is necessary — and in EFA environments, aws-ofi-nccl is required.</p> <p>NCCL was originally designed for InfiniBand and has a built-in <code class="language-plaintext highlighter-rouge">net_ib</code> transport that directly calls IB Verbs APIs. aws-ofi-nccl implements NCCL’s network plugin API and translates it to libfabric’s RDM interface.</p> <h3 id="efa-nv-peermem">efa-nv-peermem</h3> <p>This is a kernel module that enables the NIC to directly access GPU memory. The standard version is nvidia-peermem, but for EFA, efa-nv-peermem is used instead.</p> <p>By default, NICs can only read CPU memory. efa-nv-peermem bridges the GPU and NIC, enabling direct access. (It’s a GPUDirect RDMA module.)</p> </div> </article> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2026 Lucas Sangdae Nam. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>