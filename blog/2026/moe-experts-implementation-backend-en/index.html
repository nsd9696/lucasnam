<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> MoE Expert FFN Backend: experts_implementation | Lucas Sangdae Nam </title> <meta name="author" content="Lucas Sangdae Nam"> <meta name="description" content="Selecting Expert FFN computation backends (eager, batched_mm, grouped_mm) in HuggingFace Transformers and benchmarking with Solar-Open 100B"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://nsd9696.github.io/blog/2026/moe-experts-implementation-backend-en/"> <script src="/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Lucas</span> Sangdae Nam </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> <li class="toggle-container"> <a class="nav-link" href="/ko/" title="한국어">KO</a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">MoE Expert FFN Backend: experts_implementation</h1> <p class="post-meta"> Created on January 30, 2026 </p> <p class="post-tags"> <a href="/blog/2026"> <i class="fa-solid fa-calendar fa-sm"></i> 2026 </a>   ·   <a href="/blog/tag/moe"> <i class="fa-solid fa-hashtag fa-sm"></i> moe</a>   <a href="/blog/tag/experts-implementation"> <i class="fa-solid fa-hashtag fa-sm"></i> experts-implementation</a>   <a href="/blog/tag/huggingface"> <i class="fa-solid fa-hashtag fa-sm"></i> huggingface</a>   <a href="/blog/tag/transformers"> <i class="fa-solid fa-hashtag fa-sm"></i> transformers</a>   <a href="/blog/tag/torch-compile"> <i class="fa-solid fa-hashtag fa-sm"></i> torch-compile</a>   <a href="/blog/tag/grouped-gemm"> <i class="fa-solid fa-hashtag fa-sm"></i> grouped-gemm</a>   ·   <a href="/blog/category/ml-engineering"> <i class="fa-solid fa-tag fa-sm"></i> ml-engineering</a> </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h2"><a href="#1-experts_implementation">1. experts_implementation</a></li> <li class="toc-entry toc-h2"> <a href="#2-eager-batched_mm-grouped_mm">2. eager, batched_mm, grouped_mm</a> <ul> <li class="toc-entry toc-h3"><a href="#eager-loop-based-reference-implementation">eager: Loop-Based Reference Implementation</a></li> <li class="toc-entry toc-h3"><a href="#batched_mm">batched_mm</a></li> <li class="toc-entry toc-h3"><a href="#grouped_mm">grouped_mm</a></li> </ul> </li> <li class="toc-entry toc-h2"><a href="#3-solar-open-100b-benchmark">3. Solar-Open 100B Benchmark</a></li> <li class="toc-entry toc-h2"><a href="#4-wrapping-up">4. Wrapping Up</a></li> </ul> </div> <hr> <div id="markdown-content"> <h2 id="1-experts_implementation">1. experts_implementation</h2> <p>A <a href="https://github.com/huggingface/transformers/pull/42697" rel="external nofollow noopener" target="_blank">PR (#42697)</a> that adds support for selecting the Expert FFN computation method in MoE models has been merged into HuggingFace Transformers. Just like how <code class="language-plaintext highlighter-rouge">attn_implementation</code> allowed you to choose the attention computation backend, you can now hook into the expert computation and run it with the backend of your choice.</p> <hr> <h2 id="2-eager-batched_mm-grouped_mm">2. eager, batched_mm, grouped_mm</h2> <p>Fundamentally, expert FFN follows the same logic: the router selects top-k experts, performs hidden state projection with expert parameters (<code class="language-plaintext highlighter-rouge">gate_up_proj</code>, <code class="language-plaintext highlighter-rouge">down_proj</code>), and then computes a weighted sum using routing weights. The key difference lies in <strong>how the per-expert matrix multiplications are performed</strong>.</p> <h3 id="eager-loop-based-reference-implementation">eager: Loop-Based Reference Implementation</h3> <p>This is the most intuitive approach. It iterates through activated experts one by one using a Python loop, selects only the tokens routed to each expert, and performs per-expert projection on those tokens. However, because it uses <code class="language-plaintext highlighter-rouge">torch.where</code> to select tokens assigned to each expert, it becomes difficult to use with <code class="language-plaintext highlighter-rouge">torch.compile</code> with the <code class="language-plaintext highlighter-rouge">fullgraph=True</code> option.</p> <h3 id="batched_mm">batched_mm</h3> <p><code class="language-plaintext highlighter-rouge">batched_mm</code> duplicates the selected expert’s weights for each token, stacks them into a 3D tensor, and performs batched matrix multiplication all at once using <code class="language-plaintext highlighter-rouge">torch.bmm</code>.</p> <p><code class="language-plaintext highlighter-rouge">torch.bmm</code> stands for Batched Matrix Multiplication — it multiplies pairs of identically-sized matrices all at once.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Routing result (top_k=3):
  token0 → Expert0, Expert2, Expert5
  token1 → Expert1, Expert2, Expert3

┌──────────────────────────────────────────────────────────────
│  1) Duplicate expert weights for each (token, expert) pair
│
│  token0 × Expert0 weight copy ─┐
│  token0 × Expert2 weight copy ─┤
│  token0 × Expert5 weight copy ─┤  ← Stack into 3D tensor
│  token1 × Expert1 weight copy ─┤
│  token1 × Expert2 weight copy ─┤
│  token1 × Expert3 weight copy ─┘
│
│  ⚠️ batch_size × seq_len × top_k copies → memory increase
├──────────────────────────────────────────────────────────────
│  2) Process everything in a single torch.bmm call
│
│  [token0] × [Expert0 W] → [out0_e0] ┐
│  [token0] × [Expert2 W] → [out0_e2] ├─ token0: weighted sum of
│  [token0] × [Expert5 W] → [out0_e5] ┘  3 results by routing weight
│
│  [token1] × [Expert1 W] → [out1_e1] ┐
│  [token1] × [Expert2 W] → [out1_e2] ├─ token1: weighted sum of
│  [token1] × [Expert3 W] → [out1_e3] ┘  3 results by routing weight
│
│  → Single bmm call (batch size = num_tokens × top_k = 6)
└──────────────────────────────────────────────────────────────
</code></pre></div></div> <p>Since it is compatible with <code class="language-plaintext highlighter-rouge">torch.compile</code>, it supports <code class="language-plaintext highlighter-rouge">fullgraph</code>. However, because it copies expert weights, memory usage can more than double compared to eager — making it more advantageous for short sequences or small batch sizes.</p> <h3 id="grouped_mm">grouped_mm</h3> <p><code class="language-plaintext highlighter-rouge">grouped_mm</code> uses <code class="language-plaintext highlighter-rouge">torch._grouped_mm</code> to support Grouped GEMM. This approach does not copy weights. Instead, it groups tokens by expert and processes all expert projections simultaneously using the Grouped GEMM kernel.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Routing result (top_k=3):
  token0 → Expert0, Expert2, Expert5
  token1 → Expert1, Expert2, Expert3

┌──────────────────────────────────────────────────────────────
│  1) Sort tokens by expert
│
│  Original: [(t0,E0), (t0,E2), (t0,E5), (t1,E1), (t1,E2), (t1,E3)]
│                    ↓ sort by expert
│  Sorted: [(t0,E0) | (t1,E1) | (t0,E2),(t1,E2) | (t1,E3) | (t0,E5)]
│          ├ E0:1 ┤├ E1:1 ┤├──── E2:2 ────┤├ E3:1 ┤├ E5:1 ┤
│
│  group_sizes = [1, 1, 2, 1, 1]
├──────────────────────────────────────────────────────────────
│  2) Process with a single Grouped GEMM call (no weight copy)
│
│  [token0] × [Expert0 W] → [out0_e0] ┐
│  [token1] × [Expert1 W] → [out1_e1] │
│  [token0] × [Expert2 W] → [out0_e2] │ ← Single Grouped GEMM
│  [token1] × [Expert2 W] → [out1_e2] │
│  [token1] × [Expert3 W] → [out1_e3] │
│  [token0] × [Expert5 W] → [out0_e5] ┘
│           ↑ E2 weight shared by 2 tokens without duplication
├──────────────────────────────────────────────────────────────
│  3) Restore results to original token order, weighted sum
│
│  token0: out0_e0 × w0 + out0_e2 × w1 + out0_e5 × w2 → final0
│  token1: out1_e1 × w0 + out1_e2 × w1 + out1_e3 × w2 → final1
└──────────────────────────────────────────────────────────────
</code></pre></div></div> <p>Since weights are not duplicated, this approach is the most memory-efficient, and it particularly excels with long sequences and large batches.</p> <hr> <h2 id="3-solar-open-100b-benchmark">3. Solar-Open 100B Benchmark</h2> <p>Since the PR showed significant performance differences, I ran benchmarks directly using Upstage’s Solar-Open model. Looking at just the Mean Latency, the differences were meaningful. <code class="language-plaintext highlighter-rouge">batched_mm</code> performed reasonably well with short, small inputs, but overall <code class="language-plaintext highlighter-rouge">grouped_mm</code> showed the best performance. Compared to eager, even without compile it was about 4x faster on average, and with compile applied the latency difference reached up to 10x.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/solar_latency_summary-480.webp 480w,/assets/img/solar_latency_summary-800.webp 800w,/assets/img/solar_latency_summary-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/solar_latency_summary.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Solar-Open 100B Latency Comparison (Experts Backend &amp; Torch Compile)</figcaption> </figure> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span>
    <span class="sh">"</span><span class="s">upstage/solar-open-...</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">experts_implementation</span><span class="o">=</span><span class="sh">"</span><span class="s">grouped_mm</span><span class="sh">"</span><span class="p">,</span>  <span class="c1"># or "batched_mm", "eager"
</span><span class="p">)</span>
</code></pre></div></div> <p>Note that with <code class="language-plaintext highlighter-rouge">batched_mm</code> under <code class="language-plaintext highlighter-rouge">batch_size=4</code>, <code class="language-plaintext highlighter-rouge">seq_len=128</code> conditions, a memory spike was observed during computation in both compile default and no-compile cases. I left the results as-is without separate modifications for reference.</p> <hr> <h2 id="4-wrapping-up">4. Wrapping Up</h2> <p>It would be worth considering these backend options for MoE model inference. However, since vLLM has its own <code class="language-plaintext highlighter-rouge">fused_moe</code> kernel, it cannot be directly used there. I’ll cover the <code class="language-plaintext highlighter-rouge">fused_moe</code> kernel in a future post.</p> </div> </article> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2026 Lucas Sangdae Nam. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>