<!DOCTYPE html> <html lang="ko"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Multi-Node P/D Disagg vLLM Serving: How EFA Works Compared to InfiniBand? | Lucas Sangdae Nam </title> <meta name="author" content="Lucas Sangdae Nam"> <meta name="description" content="AWS EFA 환경에서의 멀티노드 GPU 통신, InfiniBand vs EFA 비교, vLLM P/D Disagg 구성까지"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8C%89&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://nsd9696.github.io/ko/blog/2026/multi-node-gpu-networking-efa-srd/"> <script src="/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/ko/"> <span class="font-weight-bold">Lucas</span> Sangdae Nam </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="메뉴 전환"> <span class="sr-only">메뉴 전환</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/ko/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/ko/blog/">blog <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/ko/publications/">publications </a> </li> <li class="nav-item"> <button id="search-toggle" title="검색" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="테마 변경"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> <li class="toggle-container"> <a class="nav-link" href="/" title="English">EN</a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Multi-Node P/D Disagg vLLM Serving: How EFA Works Compared to InfiniBand?</h1> <p class="post-meta"> 작성일 February 22, 2026 </p> <p class="post-tags"> <i class="fa-solid fa-calendar fa-sm"></i> 2026   ·   <i class="fa-solid fa-hashtag fa-sm"></i> gpu   <i class="fa-solid fa-hashtag fa-sm"></i> networking   <i class="fa-solid fa-hashtag fa-sm"></i> efa   <i class="fa-solid fa-hashtag fa-sm"></i> infiniband   <i class="fa-solid fa-hashtag fa-sm"></i> rdma   <i class="fa-solid fa-hashtag fa-sm"></i> vllm   ·   <i class="fa-solid fa-tag fa-sm"></i> infrastructure </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h2"> <a href="#1-%EB%A9%80%ED%8B%B0%EB%85%B8%EB%93%9C-gpu-%EC%9A%94%EA%B5%AC%EC%82%AC%ED%95%AD">1. 멀티노드 GPU 요구사항</a> <ul> <li class="toc-entry toc-h3"><a href="#tcpip%EC%9D%98-%ED%95%9C%EA%B3%84">TCP/IP의 한계</a></li> <li class="toc-entry toc-h3"><a href="#infiniband">InfiniBand</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#2-efa%EC%99%80-srd">2. EFA와 SRD</a> <ul> <li class="toc-entry toc-h3"><a href="#efa-elastic-fabric-adapter">EFA (Elastic Fabric Adapter)</a></li> <li class="toc-entry toc-h3"><a href="#%EC%9D%B4%EB%8D%94%EB%84%B7-vs-infiniband">이더넷 vs InfiniBand</a></li> <li class="toc-entry toc-h3"><a href="#rdma-%EC%A0%84%EC%86%A1-%EB%AA%A8%EB%93%9C">RDMA 전송 모드</a></li> <li class="toc-entry toc-h3"><a href="#rd%EA%B0%80-%EC%8B%A4%EC%A0%9C%EB%A1%9C-%EC%82%AC%EC%9A%A9%EB%90%98%EC%A7%80-%EC%95%8A%EB%8A%94-%EC%9D%B4%EC%9C%A0">RD가 실제로 사용되지 않는 이유</a></li> <li class="toc-entry toc-h3"><a href="#srd-scalable-reliable-datagram">SRD (Scalable Reliable Datagram)</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#3-efa--gpudirect-rdma">3. EFA → GPUDirect RDMA</a> <ul> <li class="toc-entry toc-h3"><a href="#infiniband-vs-efa-%EC%84%B1%EB%8A%A5-%EB%B9%84%EA%B5%90">InfiniBand vs EFA 성능 비교</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#4-efa-%EC%9C%84%EC%97%90%EC%84%9C-pd-disagg-vllm-serving">4. EFA 위에서 P/D Disagg vLLM Serving</a> <ul> <li class="toc-entry toc-h3"><a href="#kv-cache-transfer-%EC%86%8C%ED%94%84%ED%8A%B8%EC%9B%A8%EC%96%B4-%EC%8A%A4%ED%83%9D">KV Cache Transfer 소프트웨어 스택</a></li> <li class="toc-entry toc-h3"><a href="#nixl-nvidia-inference-xfer-library">NIXL (NVIDIA Inference Xfer Library)</a></li> <li class="toc-entry toc-h3"><a href="#ucx-unified-communication-x">UCX (Unified Communication X)</a></li> <li class="toc-entry toc-h3"><a href="#libfabric-open-fabrics-interface">libfabric (Open Fabrics Interface)</a></li> <li class="toc-entry toc-h3"><a href="#aws-ofi-nccl-optional">aws-ofi-nccl (optional)</a></li> <li class="toc-entry toc-h3"><a href="#efa-nv-peermem">efa-nv-peermem</a></li> </ul> </li> </ul> </div> <hr> <div id="markdown-content"> <h2 id="1-멀티노드-gpu-요구사항">1. 멀티노드 GPU 요구사항</h2> <p>단일노드에서 GPU 끼리 통신할 때는 NVLink가 초당 수백 GB 대역폭을 제공합니다. NVLink 는 GPU 간의 직접 연결 (point-to-point)을 가능하게 해서 CPU 를 거치지 않고 GPU 끼리 직접 통신을 가능하게 합니다.</p> <p>문제는 멀티노드로 가면서 노드간 네트워크로 병목이 옮겨집니다. 특히 LLM 서빙에서 Tensor Parallelism 의 AllReduce 나 Disaggregated Serving(Prefill / Decode 분리) 에서의 KV cache transfer 가 그것이죠. 이때 당연히 TCP/IP 는 요구를 충족하지 못합니다.</p> <h3 id="tcpip의-한계">TCP/IP의 한계</h3> <p>TCP 는 매 패킷마다 커널 네트워크 스택들을 통과해야 합니다. 이때 system call, context switch, protocol 처리, buffer copy 같은게 연쇄적으로 발생해서 아주아주 느려지죠. 이 과정에서 불필요한 memcopy 도 같이 발생합니다.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>TCP/IP 경로 (기존):
GPU → cudaMemcpy → CPU RAM → send() → 커널 → NIC
  → 네트워크 →
NIC → 커널 → recv() → CPU RAM → cudaMemcpy → GPU

RDMA + GPUDirect 경로:
GPU HBM → NIC (GPU 메모리 직접 DMA)
  → 네트워크 →
NIC → GPU HBM (직접 DMA)
</code></pre></div></div> <p>RDMA(Remote Direct Memory Access)는 CPU와 커널을 우회하여 NIC가 직접 메모리에 접근할 수 있게 합니다. OS Bypass를 통해 system call과 context switch 오버헤드를 제거하고, GPUDirect RDMA는 여기서 한 발 더 나아가 NIC가 GPU 메모리(HBM)에 직접 DMA를 수행할 수 있게 합니다.</p> <h3 id="infiniband">InfiniBand</h3> <p>이런 GPUDirect RDMA 까지 가능하게 한 것이 InfiniBand이고 HDR(A100 표준), NDR(H100 표준)과 같은 세대에 따라 포트당 대역폭이 약 2배 정도씩(200Gbps → 400Gbps) 더 증가한다고 보시면 될 것 같습니다.</p> <hr> <h2 id="2-efa와-srd">2. EFA와 SRD</h2> <h3 id="efa-elastic-fabric-adapter">EFA (Elastic Fabric Adapter)</h3> <p>EFA(Elastic Fabric Adapter)는 AWS 에서 설계한 고성능 네트워크 인터페이스인데 InfiniBand 같은거라고 보시면 될 것 같습니다. 특정 instance type들(p4d, p5, p6 등) 에서 사용할 수 있고 위에서 설명드린 OS bypass 나 RDMA의 기능들을 제공합니다.</p> <h3 id="이더넷-vs-infiniband">이더넷 vs InfiniBand</h3> <p>EFA의 가장 큰 차이점은 표준 이더넷 패브릭 위에서 동작한다는 점입니다. InfiniBand 는 NVIDIA(Mellanox) 단일 벤더입니다. 그래서 스위치나 NIC, 케이블 모두 Mellanox 제품이어야 하고 대안이 거의 없습니다. 반대로 이더넷 장비는 Broadcom, Intel 등의 다수 벤더가 가능한 범용적인 장비입니다.</p> <h3 id="rdma-전송-모드">RDMA 전송 모드</h3> <table> <thead> <tr> <th>모드</th> <th>전용 연결 필요</th> <th>전달 보장</th> <th>순서 보장</th> <th>설명</th> </tr> </thead> <tbody> <tr> <td> <strong>RC</strong> (Reliable Connection)</td> <td>필요 (서버마다 전용 QP)</td> <td>O</td> <td>O</td> <td>InfiniBand 기본 전송 방식</td> </tr> <tr> <td> <strong>UD</strong> (Unreliable Datagram)</td> <td>불필요 (하나의 QP)</td> <td>X</td> <td>X</td> <td>가장 가벼운 방식</td> </tr> <tr> <td> <strong>RD</strong> (Reliable Datagram)</td> <td>불필요 (하나의 QP)</td> <td>O</td> <td>O</td> <td>이론적으로 이상적이나 미구현</td> </tr> <tr> <td> <strong>SRD</strong> (Scalable Reliable Datagram)</td> <td>불필요</td> <td>O</td> <td>X (SW 재정렬)</td> <td>AWS가 설계한 방식</td> </tr> </tbody> </table> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>RC (Reliable Connection):
서버 A ──전용 회선──→ 서버 B
서버 A ──전용 회선──→ 서버 C
→ 서버마다 전용 QP(Queue Pair)가 필요

UD (Unreliable Datagram):
서버 A ──회선──→ 아무에게나 보낼 수 있음
→ 전용 회선 불필요, 하나의 QP로 모든 상대와 통신

RD (Reliable Datagram):
서버 A ──회선──→ 아무에게나 보낼 수 있음
→ 전용 회선 불필요 + 전달 보장 + 순서 보장
</code></pre></div></div> <h3 id="rd가-실제로-사용되지-않는-이유">RD가 실제로 사용되지 않는 이유</h3> <p>다만, InfiniBand 에서 실질적으로 RD 를 잘 사용하고 있지는 않습니다. 이유는 결국 하드웨어 구현 난이도 인데요.</p> <ul> <li> <strong>RC</strong> 는 QP 하나당 상태를 하나만 추적하면 되고, <strong>UD</strong>는 아무에게나 보내는 것이기 때문에 사실상 상태 추적을 할 필요가 없습니다.</li> <li>하지만 <strong>RD</strong>의 NIC 는 모든 통신 상대별 상태를 추적해야 합니다: <ul> <li>“서버 B에게 패킷 3까지 보냈고 ACK 2까지 받음”</li> <li>“서버 C에게 패킷 7까지 보냈고 ACK 5까지 받음”</li> <li>“서버 D에게 패킷 1 보냈고 ACK 아직 없음”</li> <li>“서버 E에게 패킷 12까지 보냈고 ACK 10까지 받음”</li> <li>… × 통신하는 모든 상대방</li> </ul> </li> <li>예를 들어, 하나의 QP가 1000개의 서버와 통신하면 1000개의 상태를 NIC 에서 모두 관리해야 합니다. 이때 NIC 칩 안의 SRAM 에 저장해야 하는데 이게 비싸고 작습니다.</li> <li>따라서 NVIDIA 입장에서는 RC 나 UD 로도 커버가 가능한데 RD까지의 수요가 없었고 비용 대비 효과가 안 맞아서 deprecate 합니다.</li> </ul> <h3 id="srd-scalable-reliable-datagram">SRD (Scalable Reliable Datagram)</h3> <p>그러면 AWS 에서 만든 SRD는 뭐가 다른가? <strong>RD 에서의 순서 보장을 포기했습니다.</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>RD:   패킷 1 → 패킷 2 → 패킷 3  (반드시 이 순서로 도착)
      경로: A ━━━━━━━━━━━━━→ B  (하나의 경로)

SRD:  패킷 1 ──경로 A──→ ┐
      패킷 2 ──경로 B──→ ├→ 도착 후 소프트웨어가 재정렬
      패킷 3 ──경로 C──→ ┘
</code></pre></div></div> <p>순서를 포기하면 여러 경로에 패킷을 분산시킬 수 있고, 한 경로가 막혀도 다른 경로로 보내면 되기 때문에 이더넷 환경에서 유리합니다. 수천 대의 서버가 네트워크를 공유하는 환경에서는 이런 SRD 가 안정적이기 때문에 채택했다고 보시면 될 것 같습니다.</p> <hr> <h2 id="3-efa--gpudirect-rdma">3. EFA → GPUDirect RDMA</h2> <p>EFA 를 사용했을 때 GPUDirect RDMA 흐름은 다음과 같습니다.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/efa_blog_1-480.webp 480w,/assets/img/efa_blog_1-800.webp 800w,/assets/img/efa_blog_1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/efa_blog_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">EFA GPUDirect RDMA Flow</figcaption> </figure> <p>물론 이때 각 노드별 GPU 끼리의 통신에는 NVLink를 사용합니다.</p> <h3 id="infiniband-vs-efa-성능-비교">InfiniBand vs EFA 성능 비교</h3> <p>다만, EFA 가 InfiniBand 와 비교했을 때 performance 가 나오는가? 이를 비교하는 벤치마크를 요약하면 다음과 같습니다.</p> <table> <thead> <tr> <th>항목</th> <th>InfiniBand</th> <th>EFA/이더넷</th> <th>결론</th> </tr> </thead> <tbody> <tr> <td><strong>소규모 메시지 지연</strong></td> <td>~1 µs</td> <td>~10 µs</td> <td>IB 압도적 우위</td> </tr> <tr> <td><strong>대용량 전송 대역폭</strong></td> <td>~200 Gbps</td> <td>~200 Gbps</td> <td>비슷</td> </tr> <tr> <td><strong>AI 학습 (대규모)</strong></td> <td>기준선</td> <td>적절한 튜닝 시 유사</td> <td>차이 미미</td> </tr> <tr> <td><strong>AI 추론 (Decode)</strong></td> <td>유리</td> <td>평균 1.0166% 느림</td> <td>IB 약간 유리</td> </tr> <tr> <td><strong>비용</strong></td> <td>1.5~2.5배 비쌈</td> <td>기준선</td> <td>이더넷 유리</td> </tr> </tbody> </table> <blockquote> <p>출처: <a href="https://www.wwt.com/blog/the-battle-of-ai-networking-ethernet-vs-infiniband" rel="external nofollow noopener" target="_blank">WWT - The Battle of AI Networking</a>, <a href="https://www.vitextech.com/blogs/blog/infiniband-vs-ethernet-for-ai-clusters-effective-gpu-networks-in-2025" rel="external nofollow noopener" target="_blank">Vitex Tech - InfiniBand vs Ethernet</a></p> </blockquote> <p>전반적으로 성능 자체는 InfiniBand가 좋은 것으로 보입니다. 다만, AWS 환경에 셋업이 되어 있는 것을 고려한다면 EFA가 유리할 수 있지만 최근 neo-cloud GPU 가격이 AWS 에 비해 저렴하게 많이 나오는 편인 것을 고려하면 InfiniBand가 더 나은 선택일 수 있을 것 같습니다.</p> <hr> <h2 id="4-efa-위에서-pd-disagg-vllm-serving">4. EFA 위에서 P/D Disagg vLLM Serving</h2> <p>저의 경우 이번에 작업하던 내용이 A100 환경에서 Prefill/Decode Disagg 를 구성하는 것이였습니다. 해당 경우, EFA 가 직접적으로 관여하는 구간은 kv_transfer 입니다.</p> <h3 id="kv-cache-transfer-소프트웨어-스택">KV Cache Transfer 소프트웨어 스택</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/efa_blog_2-480.webp 480w,/assets/img/efa_blog_2-800.webp 800w,/assets/img/efa_blog_2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/efa_blog_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">KV Cache Transfer Software Stack on EFA</figcaption> </figure> <p>이제 EFA 위의 각각의 구성요소들을 구체적으로 봐보겠습니다.</p> <h3 id="nixl-nvidia-inference-xfer-library">NIXL (NVIDIA Inference Xfer Library)</h3> <p>NIXL은 GPU간의 메모리 전송을 위한 전용 라이브러리입니다. 기존의 NCCL 은 AllReduce 나 All-to-All 과 같은 집합 통신에는 특화되어 있는데, 특정 GPU → GPU 로 메모리 블록을 직접 옮기는 point-to-point 전송에는 적합하지 않아서 이를 위해 만들어진 것이 NIXL 입니다.</p> <p>P/D Disagg 에서의 <code class="language-plaintext highlighter-rouge">Prefill GPU ──RDMA Write──▶ Decode GPU</code> 과정이나, Decode 인스턴스 간의 마이그레이션 <code class="language-plaintext highlighter-rouge">Decode GPU (서버 A, 과부하) ──RDMA──▶ Decode GPU (서버 B, 여유)</code> 이런 경우에서 사용하게 됩니다.</p> <p>이때, NIXL 에는 NIXL Agent 가 있는데 메모리 등록이나, 메타데이터(GPU 메모리 주소, RDMA key, NIC 주소 등) 관리, 실제 전송을 수행하는 플러그인 백엔드(UCX, libfabric) 등을 관리합니다.</p> <h3 id="ucx-unified-communication-x">UCX (Unified Communication X)</h3> <p>본래 InfiniBand 를 위한 범용 통신 프레임워크이며 내부적으로 RC, UD, TCP, cuda_ipc (같은 노드 GPU 간), cuda_copy(GPU ↔ CPU 복사) 등의 프로토콜을 지원합니다.</p> <p>다만, EFA 에서는 <code class="language-plaintext highlighter-rouge">UCX_TLS=ib</code> 를 설정하면 사용이 가능한데, 이는 EFA가 ibverbs 호환 인터페이스를 제공하기 때문입니다.</p> <ul> <li><a href="https://github.com/amzn/amzn-drivers/blob/master/kernel/linux/efa/src/efa_verbs.c" rel="external nofollow noopener" target="_blank">EFA ibverbs 구현 (AWS driver)</a></li> <li><a href="https://github.com/linux-rdma/rdma-core/blob/master/providers/efa/efa.c" rel="external nofollow noopener" target="_blank">rdma-core EFA provider</a></li> </ul> <h3 id="libfabric-open-fabrics-interface">libfabric (Open Fabrics Interface)</h3> <p>네트워크 하드웨어 벤더들의 표준 추상화 계층입니다. 내부적으로 EFA provider, TCP provider, SHM provider 등으로 구성되어 있습니다.</p> <p>공식적으로는 libfabric 을 사용하는 것이 맞는 것으로 보이는데 실제 작업 과정에서는 UCX 쪽이 dependency 가 잘 맞고 libfabric 쪽에서는 GPU memory bad address 이슈가 지속적으로 발생해서 우선 저의 경우는 deprecate 했습니다.</p> <h3 id="aws-ofi-nccl-optional">aws-ofi-nccl (optional)</h3> <p>저의 경우 P/D Disagg 구성이었기 때문에 NCCL 사용 필요성이 따로 없었습니다. 다만, 모델 자체가 커서 multi-node 로 구성하거나 multi-node train의 경우 NCCL 이 필요하고 EFA 환경에서는 aws-ofi-nccl 이 필요합니다.</p> <p>본래 NCCL 은 InfiniBand 를 위해 설계되어서 IB Verbs API 를 직접 호출하는 <code class="language-plaintext highlighter-rouge">net_ib</code> 전송이 내장되어 있는데 aws-ofi-nccl 은 NCCL의 네트워크 플러그인 API 를 구현해서 이를 libfabric 의 RDM 인터페이스로 변환합니다.</p> <h3 id="efa-nv-peermem">efa-nv-peermem</h3> <p>NIC 가 GPU 메모리에 직접 접근할 수 있게 해주는 커널 모듈입니다. 본래는 nvidia-peermem 인데 EFA 에서는 efa-nv-peermem 을 사용합니다.</p> <p>기본적으로 NIC 는 CPU 메모리만 읽을 수 있기 때문에 GPU 와 NIC, 이 둘을 연결하는게 efa-nv-peermem인겁니다. (GPUDirect RDMA 모듈입니다)</p> </div> </article> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2026 Lucas Sangdae Nam. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>